{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataset\n",
    "Dataset used is celebA : https://www.kaggle.com/jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "./data/img_align_celeba/img_align_celeba; No such file or directory [Op:MatchingFiles]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f273ebf30f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mim_patern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/img_align_celeba/img_align_celeba/*.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfiles_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_patern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mim_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_preprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mim_dataset_repeated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mlist_files\u001b[0;34m(file_pattern, shuffle, seed)\u001b[0m\n\u001b[1;32m   1060\u001b[0m       file_pattern = ops.convert_to_tensor(\n\u001b[1;32m   1061\u001b[0m           file_pattern, dtype=dtypes.string, name=\"file_pattern\")\n\u001b[0;32m-> 1062\u001b[0;31m       \u001b[0mmatching_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m       \u001b[0;31m# Raise an exception if `file_pattern` does not match any files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mmatching_files\u001b[0;34m(pattern, name)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ./data/img_align_celeba/img_align_celeba; No such file or directory [Op:MatchingFiles]"
     ]
    }
   ],
   "source": [
    "IM_DIM = 64\n",
    "@tf.function\n",
    "def im_preprocessing(im_path) :\n",
    "    im_file = tf.io.read_file(im_path)\n",
    "    im = tf.io.decode_jpeg(im_file)\n",
    "    im = tf.image.convert_image_dtype(im, tf.float32)\n",
    "    #im = tf.image.crop_to_bounding_box(im, 20,0,178,178)\n",
    "    im = tf.image.resize(im, [IM_DIM, IM_DIM])\n",
    "    im = tf.image.random_flip_left_right(im)\n",
    "    return(im)\n",
    "\n",
    "batch_size = 32\n",
    "im_patern = \"./data/img_align_celeba/img_align_celeba/*.jpg\"\n",
    "files_dataset = tf.data.Dataset.list_files(im_patern)\n",
    "im_dataset = files_dataset.map(im_preprocessing)\n",
    "im_dataset_repeated = im_dataset.repeat()\n",
    "im_dataset_batch = im_dataset_repeated.batch(batch_size)\n",
    "\n",
    "for x in im_dataset.take(1) :\n",
    "    print(x.numpy().shape)\n",
    "    print(\"len dataset : \", tf.data.experimental.cardinality(im_dataset).numpy())\n",
    "    plt.imshow(x)\n",
    "    \n",
    "batch_gen = iter(im_dataset_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH = 32\n",
    "LATENT_DEPTH = 512\n",
    "K_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    mean, logsigma = args\n",
    "    epsilon = keras.backend.random_normal(shape=keras.backend.shape(mean))\n",
    "    return mean + tf.exp(logsigma / 2) * epsilon\n",
    "\n",
    "def encoder():\n",
    "    input_E = keras.layers.Input(shape=(IM_DIM, IM_DIM, 3))\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH*2, kernel_size=K_SIZE, strides=2, padding='same')(input_E)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.Dense(LATENT_DEPTH)(X)    \n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    mean = keras.layers.Dense(LATENT_DEPTH,activation=\"tanh\")(X)\n",
    "    logsigma = keras.layers.Dense(LATENT_DEPTH,activation=\"tanh\")(X)\n",
    "    latent = keras.layers.Lambda(sampling, output_shape=(LATENT_DEPTH,))([mean, logsigma])\n",
    "    \n",
    "    kl_loss = 1 + logsigma - keras.backend.square(mean) - keras.backend.exp(logsigma)\n",
    "    kl_loss = keras.backend.mean(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    return keras.models.Model(input_E, [latent,kl_loss])\n",
    "\n",
    "def generator():\n",
    "    input_G = keras.layers.Input(shape=(LATENT_DEPTH,))\n",
    "\n",
    "    X = keras.layers.Dense(8*8*DEPTH*8)(input_G)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    X = keras.layers.Reshape((8, 8, DEPTH * 8))(X)\n",
    "    \n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2DTranspose(filters=DEPTH, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=3, kernel_size=K_SIZE, padding='same')(X)\n",
    "    X = keras.layers.Activation('sigmoid')(X)\n",
    "\n",
    "    return keras.models.Model(input_G, X)\n",
    "\n",
    "def discriminator():\n",
    "    input_D = keras.layers.Input(shape=(IM_DIM, IM_DIM, 3))\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH, kernel_size=K_SIZE, strides=2, padding='same')(input_D)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Conv2D(filters=DEPTH*4, kernel_size=K_SIZE, strides=2, padding='same')(input_D)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, strides=2, padding='same')(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(filters=DEPTH*8, kernel_size=K_SIZE, padding='same')(X)\n",
    "    inner_output = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.Dense(DEPTH*8)(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    output = keras.layers.Dense(1)(X)    \n",
    "    \n",
    "    return keras.models.Model(input_D, [output, inner_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in im_dataset_batch.take(1) :\n",
    "    test_images = x\n",
    "test_r = tf.random.normal((batch_size, LATENT_DEPTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = encoder()\n",
    "G = generator()\n",
    "D = discriminator() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 14900  gan_loss 0.171 vae_loss 0.182 fake_dis_loss 0.319 r_dis_loss 0.066 t_dis_loss 0.15 vae_inner_loss 0.021 E_loss 0.034 D_loss 0.171 kl_loss 0.16 normal_loss 0.10708"
     ]
    }
   ],
   "source": [
    "lr=0.0001\n",
    "#lr=0.0001\n",
    "E_opt = keras.optimizers.Adam(lr=lr)\n",
    "G_opt = keras.optimizers.Adam(lr=lr)\n",
    "D_opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "inner_loss_coef = 1\n",
    "normal_coef = 0.1\n",
    "kl_coef = 0.01\n",
    "\n",
    "@tf.function\n",
    "def train_step_vaegan(x):\n",
    "    lattent_r =  tf.random.normal((batch_size, LATENT_DEPTH))\n",
    "    with tf.GradientTape(persistent=True) as tape :\n",
    "        lattent,kl_loss = E(x)\n",
    "        fake = G(lattent)\n",
    "        dis_fake,dis_inner_fake = D(fake)\n",
    "        dis_fake_r,_ = D(G(lattent_r))\n",
    "        dis_true,dis_inner_true = D(x)\n",
    "\n",
    "        vae_inner = dis_inner_fake-dis_inner_true\n",
    "        vae_inner = vae_inner*vae_inner\n",
    "        \n",
    "        mean,var = tf.nn.moments(E(x)[0], axes=0)\n",
    "        var_to_one = var - 1\n",
    "        \n",
    "        normal_loss = tf.reduce_mean(mean*mean) + tf.reduce_mean(var_to_one*var_to_one)\n",
    "        \n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        vae_diff_loss = tf.reduce_mean(vae_inner)\n",
    "        f_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.zeros_like(dis_fake), dis_fake))\n",
    "        r_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.zeros_like(dis_fake_r), dis_fake_r))\n",
    "        t_dis_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.ones_like(dis_true), dis_true))\n",
    "        gan_loss = (0.5*t_dis_loss + 0.25*f_dis_loss + 0.25*r_dis_loss)\n",
    "        vae_loss = tf.reduce_mean(tf.abs(x-fake)) \n",
    "        E_loss = vae_diff_loss + kl_coef*kl_loss + normal_coef*normal_loss\n",
    "        G_loss = inner_loss_coef*vae_diff_loss - gan_loss\n",
    "        D_loss = gan_loss\n",
    "    \n",
    "    E_grad = tape.gradient(E_loss,E.trainable_variables)\n",
    "    G_grad = tape.gradient(G_loss,G.trainable_variables)\n",
    "    D_grad = tape.gradient(D_loss,D.trainable_variables)\n",
    "    del tape\n",
    "    E_opt.apply_gradients(zip(E_grad, E.trainable_variables))\n",
    "    G_opt.apply_gradients(zip(G_grad, G.trainable_variables))\n",
    "    D_opt.apply_gradients(zip(D_grad, D.trainable_variables))\n",
    "\n",
    "    return [gan_loss, vae_loss, f_dis_loss, r_dis_loss, t_dis_loss, vae_diff_loss, E_loss, D_loss, kl_loss, normal_loss]\n",
    "\n",
    "step = 0\n",
    "max_step = 10000000\n",
    "log_freq,img_log_freq = 10, 100\n",
    "save_freq,save_number_mult = 1000, 10000\n",
    "\n",
    "metrics_names = [\"gan_loss\", \"vae_loss\", \"fake_dis_loss\", \"r_dis_loss\", \"t_dis_loss\", \"vae_inner_loss\", \"E_loss\", \"D_loss\", \"kl_loss\", \"normal_loss\"]\n",
    "metrics = []\n",
    "for m in metrics_names :\n",
    "    metrics.append(tf.keras.metrics.Mean('m', dtype=tf.float32))\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = ('logs/sep_D%dL%d/' % (DEPTH,LATENT_DEPTH)) + current_time \n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "name = ('sep_D%dL%d' % (DEPTH,LATENT_DEPTH))\n",
    "\n",
    "def save_model() :\n",
    "    nb = str(step // save_number_mult)\n",
    "    D.save('saved-models/D_training_' + nb + '.h5')\n",
    "    G.save('saved-models/G_training_' + nb + '.h5')\n",
    "    E.save('saved-models/E_training_' + nb + '.h5')\n",
    "\n",
    "def print_metrics() :\n",
    "    s = \"\"\n",
    "    for name,metric in zip(metrics_names,metrics) :\n",
    "        s+= \" \" + name + \" \" + str(np.around(metric.result().numpy(), 3)) \n",
    "    print(f\"\\rStep : \" + str(step) + \" \" + s, end=\"\", flush=True)\n",
    "    with train_summary_writer.as_default():\n",
    "        for name,metric in zip(metrics_names,metrics) :\n",
    "            tf.summary.scalar(name, metric.result(), step=step)\n",
    "    for metric in metrics : \n",
    "        metric.reset_states()\n",
    "\n",
    "        \n",
    "def log_images() :\n",
    "    with train_summary_writer.as_default():\n",
    "        lattent,_ = E(test_images)\n",
    "        fake = G(lattent)\n",
    "        fake_r = G(test_r)\n",
    "        tf.summary.image(\"reconstructed image\", fake[:8], step=step, max_outputs=8)\n",
    "        tf.summary.image(\"random image\", fake_r[:8], step=step, max_outputs=8)\n",
    "        dis_fake,inner_dis_fake = D(fake)\n",
    "        dis_fake_r,inner_dis_fake_r = D(fake_r)\n",
    "        dis_true,inner_dis_true = D(test_images)\n",
    "        tf.summary.histogram(\"dis fake\", inner_dis_fake, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis true\", inner_dis_true, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis random\", inner_dis_fake_r, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis lattent\", lattent, step=step, buckets=20)\n",
    "        tf.summary.histogram(\"dis normal\", tf.random.normal((batch_size, LATENT_DEPTH)), step=step, buckets=20)        \n",
    "\n",
    "for x in batch_gen :\n",
    "    step += 1\n",
    "    if not step % log_freq :\n",
    "        print_metrics()\n",
    "    if not step % img_log_freq :\n",
    "        log_images()\n",
    "    if not step % save_freq :\n",
    "        save_model()\n",
    "    \n",
    "    results = train_step_vaegan(x)\n",
    "    for metric,result in zip(metrics, results) :\n",
    "        metric(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
